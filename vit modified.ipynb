{"nbformat":4,"nbformat_minor":0,"metadata":{"colab":{"provenance":[],"authorship_tag":"ABX9TyNhyNqj+iPKdAc3K6Dpovw6"},"kernelspec":{"name":"python3","display_name":"Python 3"},"language_info":{"name":"python"}},"cells":[{"cell_type":"code","execution_count":3,"metadata":{"colab":{"base_uri":"https://localhost:8080/","height":1000},"id":"3n7agzbnfJAw","executionInfo":{"status":"error","timestamp":1761616110772,"user_tz":-360,"elapsed":3332,"user":{"displayName":"Nesar Ahmed","userId":"06024373412630389064"}},"outputId":"433a0b24-75fb-4732-ac36-0dd20586959f"},"outputs":[{"output_type":"stream","name":"stdout","text":["Loading dataset...\n","Train samples: 50000\n","Test samples: 10000\n"]},{"output_type":"stream","name":"stderr","text":["WARNING:tensorflow:AutoGraph could not transform <function <lambda> at 0x7820fd49f100> and will run it as-is.\n","Cause: could not parse the source code of <function <lambda> at 0x7820fd49f100>: found multiple definitions with identical signatures at the location. This error may be avoided by defining each lambda on a single line and with unique argument names. The matching definitions were:\n","Match 0:\n","lambda x, y: (x, y)\n","\n","Match 1:\n","lambda x, y: (x, tf.squeeze(y))\n","\n","To silence this warning, decorate the function with @tf.autograph.experimental.do_not_convert\n"]},{"output_type":"stream","name":"stdout","text":["WARNING: AutoGraph could not transform <function <lambda> at 0x7820fd49f100> and will run it as-is.\n","Cause: could not parse the source code of <function <lambda> at 0x7820fd49f100>: found multiple definitions with identical signatures at the location. This error may be avoided by defining each lambda on a single line and with unique argument names. The matching definitions were:\n","Match 0:\n","lambda x, y: (x, y)\n","\n","Match 1:\n","lambda x, y: (x, tf.squeeze(y))\n","\n","To silence this warning, decorate the function with @tf.autograph.experimental.do_not_convert\n"]},{"output_type":"stream","name":"stderr","text":["WARNING:tensorflow:AutoGraph could not transform <function <lambda> at 0x7820fd49f1a0> and will run it as-is.\n","Cause: could not parse the source code of <function <lambda> at 0x7820fd49f1a0>: found multiple definitions with identical signatures at the location. This error may be avoided by defining each lambda on a single line and with unique argument names. The matching definitions were:\n","Match 0:\n","lambda x, y: (x, y)\n","\n","Match 1:\n","lambda x, y: (x, tf.squeeze(y))\n","\n","To silence this warning, decorate the function with @tf.autograph.experimental.do_not_convert\n"]},{"output_type":"stream","name":"stdout","text":["WARNING: AutoGraph could not transform <function <lambda> at 0x7820fd49f1a0> and will run it as-is.\n","Cause: could not parse the source code of <function <lambda> at 0x7820fd49f1a0>: found multiple definitions with identical signatures at the location. This error may be avoided by defining each lambda on a single line and with unique argument names. The matching definitions were:\n","Match 0:\n","lambda x, y: (x, y)\n","\n","Match 1:\n","lambda x, y: (x, tf.squeeze(y))\n","\n","To silence this warning, decorate the function with @tf.autograph.experimental.do_not_convert\n","\n","========================================\n","Running experiment: baseline_vit\n","========================================\n"]},{"output_type":"error","ename":"TypeError","evalue":"Layer.add_weight() got multiple values for argument 'shape'","traceback":["\u001b[0;31m---------------------------------------------------------------------------\u001b[0m","\u001b[0;31mTypeError\u001b[0m                                 Traceback (most recent call last)","\u001b[0;32m/tmp/ipython-input-3962297969.py\u001b[0m in \u001b[0;36m<cell line: 0>\u001b[0;34m()\u001b[0m\n\u001b[1;32m    349\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    350\u001b[0m         \u001b[0;31m# Build model with given hyperparameters\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 351\u001b[0;31m         model = create_improved_vit(\n\u001b[0m\u001b[1;32m    352\u001b[0m             \u001b[0minput_shape\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;36m32\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;36m32\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;36m3\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    353\u001b[0m             \u001b[0mimage_size\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mIMAGE_SIZE\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n","\u001b[0;32m/tmp/ipython-input-3962297969.py\u001b[0m in \u001b[0;36mcreate_improved_vit\u001b[0;34m(input_shape, image_size, patch_size, projection_dim, num_heads, transformer_layers, transformer_mlp_units, mlp_head_units, drop_path_rate, num_classes)\u001b[0m\n\u001b[1;32m    236\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    237\u001b[0m     \u001b[0;31m# Encode patches with class token + position embeddings\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 238\u001b[0;31m     \u001b[0mencoded_patches\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mPatchEncoder\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mNUM_PATCHES\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mprojection_dim\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mpatches\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    239\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    240\u001b[0m     \u001b[0;31m# Transformer blocks with optional stochastic depth (DropPath)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n","\u001b[0;32m/usr/local/lib/python3.12/dist-packages/keras/src/utils/traceback_utils.py\u001b[0m in \u001b[0;36merror_handler\u001b[0;34m(*args, **kwargs)\u001b[0m\n\u001b[1;32m    120\u001b[0m             \u001b[0;31m# To get the full stack trace, call:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    121\u001b[0m             \u001b[0;31m# `keras.config.disable_traceback_filtering()`\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 122\u001b[0;31m             \u001b[0;32mraise\u001b[0m \u001b[0me\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mwith_traceback\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mfiltered_tb\u001b[0m\u001b[0;34m)\u001b[0m \u001b[0;32mfrom\u001b[0m \u001b[0;32mNone\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    123\u001b[0m         \u001b[0;32mfinally\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    124\u001b[0m             \u001b[0;32mdel\u001b[0m \u001b[0mfiltered_tb\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n","\u001b[0;32m/tmp/ipython-input-3962297969.py\u001b[0m in \u001b[0;36mbuild\u001b[0;34m(self, input_shape)\u001b[0m\n\u001b[1;32m    171\u001b[0m     \u001b[0;32mdef\u001b[0m \u001b[0mbuild\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0minput_shape\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    172\u001b[0m         \u001b[0;31m# create a trainable class token (1, 1, projection_dim)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 173\u001b[0;31m         \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mclass_token\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0madd_weight\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m'class_token'\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mshape\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;36m1\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;36m1\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mprojection\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0munits\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0minitializer\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;34m'zeros'\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mtrainable\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;32mTrue\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    174\u001b[0m         \u001b[0msuper\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mbuild\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0minput_shape\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    175\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n","\u001b[0;31mTypeError\u001b[0m: Layer.add_weight() got multiple values for argument 'shape'"]}],"source":["\"\"\"\n","ViT_CIFAR_Experiments.py\n","\n","Runnable script / notebook-style Python file implementing an improved Vision Transformer (ViT)\n","with multiple experiments on CIFAR-10 and CIFAR-100.\n","\n","How to run:\n","- Recommended: open in Google Colab (Runtime > Change runtime type > GPU)\n","- Run the whole file cell-by-cell as a notebook, or run as a script in an environment with a GPU.\n","\n","What this file contains:\n","- Data loading (CIFAR-10 and CIFAR-100 switchable)\n","- Data augmentation & preprocessing\n","- Improved ViT model (Conv stem, Class token, DropPath, Cosine LR)\n","- Experiment loop to try different hyperparameters\n","- Training & validation plots, test evaluation\n","- Results CSV + comparison table printed\n","- Inline explanations (as comments) for every major modification\n","\n","Author: Generated for user's assignment by ChatGPT (GPT-5 Thinking mini).\n","\"\"\"\n","\n","# Notebook-style imports\n","import os\n","import math\n","import json\n","import time\n","from datetime import datetime\n","\n","import numpy as np\n","import pandas as pd\n","import matplotlib.pyplot as plt\n","\n","import tensorflow as tf\n","from tensorflow import keras\n","from tensorflow.keras import layers\n","\n","# -----------------------------\n","# User-config / quick switches\n","# -----------------------------\n","USE_CIFAR = 100  # set to 10 for CIFAR-10, 100 for CIFAR-100\n","RUN_EXPERIMENTS = True  # set False to only build model\n","OUTPUT_DIR = '/content/vit_experiments'  # Colab-friendly path\n","os.makedirs(OUTPUT_DIR, exist_ok=True)\n","\n","# -----------------------------\n","# Helper: Plotting utilities\n","# -----------------------------\n","\n","def plot_history(history, title_prefix=''):\n","    # history: keras History object\n","    keys = [k for k in history.history.keys()]\n","    # plot loss\n","    plt.figure(figsize=(8, 4))\n","    plt.plot(history.history['loss'], label='train_loss')\n","    if 'val_loss' in history.history:\n","        plt.plot(history.history['val_loss'], label='val_loss')\n","    plt.xlabel('Epoch')\n","    plt.ylabel('Loss')\n","    plt.title(f'{title_prefix} Loss')\n","    plt.legend()\n","    plt.grid(True)\n","    plt.show()\n","\n","    # plot accuracy if present\n","    if 'accuracy' in history.history:\n","        plt.figure(figsize=(8, 4))\n","        plt.plot(history.history['accuracy'], label='train_acc')\n","        if 'val_accuracy' in history.history:\n","            plt.plot(history.history['val_accuracy'], label='val_acc')\n","        plt.xlabel('Epoch')\n","        plt.ylabel('Accuracy')\n","        plt.title(f'{title_prefix} Accuracy')\n","        plt.legend()\n","        plt.grid(True)\n","        plt.show()\n","\n","# -----------------------------\n","# Data loading + preprocessing\n","# -----------------------------\n","print('Loading dataset...')\n","if USE_CIFAR == 10:\n","    (x_train, y_train), (x_test, y_test) = keras.datasets.cifar10.load_data()\n","    num_classes = 10\n","else:\n","    (x_train, y_train), (x_test, y_test) = keras.datasets.cifar100.load_data()\n","    num_classes = 100\n","\n","print('Train samples:', x_train.shape[0])\n","print('Test samples:', x_test.shape[0])\n","\n","# We will resize images to a slightly larger size for the ViT patching\n","IMAGE_SIZE = 72  # you can change this to 96 if GPU allows\n","PATCH_SIZE = 6\n","NUM_PATCHES = (IMAGE_SIZE // PATCH_SIZE) ** 2\n","\n","# Simple preprocessing: keep them as uint8 until augmentation layer handles scaling.\n","# Create tf.data datasets to speed up training\n","BATCH_SIZE = 128\n","AUTOTUNE = tf.data.AUTOTUNE\n","\n","train_ds = tf.data.Dataset.from_tensor_slices((x_train, y_train))\n","val_count = int(0.1 * x_train.shape[0])\n","\n","# Shuffle then split: we'll use first val_count as validation for reproducibility\n","train_ds = train_ds.shuffle(buffer_size=10000, seed=42)\n","val_ds = train_ds.take(val_count)\n","train_ds = train_ds.skip(val_count)\n","\n","# Augmentation + preprocessing layer (as a keras Sequential so it's serializable)\n","# Note: we use Rescaling(1./255) to convert to [0,1]\n","DATA_AUG = keras.Sequential([\n","    layers.Rescaling(1.0 / 255),\n","    layers.Resizing(IMAGE_SIZE, IMAGE_SIZE),\n","    layers.RandomFlip('horizontal'),\n","    layers.RandomRotation(0.05),\n","    layers.RandomZoom(height_factor=0.12, width_factor=0.12),\n","], name='data_augmentation')\n","\n","# Prefetch datasets\n","train_ds = train_ds.batch(BATCH_SIZE).map(lambda x, y: (x, tf.squeeze(y))).map(lambda x, y: (x, y), num_parallel_calls=AUTOTUNE).prefetch(AUTOTUNE)\n","val_ds = val_ds.batch(BATCH_SIZE).map(lambda x, y: (x, tf.squeeze(y))).prefetch(AUTOTUNE)\n","\n","# Test dataset\n","test_ds = tf.data.Dataset.from_tensor_slices((x_test, y_test)).batch(BATCH_SIZE).map(lambda x, y: (x, tf.squeeze(y))).prefetch(AUTOTUNE)\n","\n","# -----------------------------\n","# Model components & improvements (with explanations)\n","# -----------------------------\n","\n","# 1) Convolutional stem: small conv block before patching helps capture local features early.\n","def conv_stem(x):\n","    # 3x3 conv -> BN -> GELU, then another downsample conv to slightly increase receptive field\n","    x = layers.Conv2D(32, kernel_size=3, padding='same', activation=None)(x)\n","    x = layers.BatchNormalization()(x)\n","    x = layers.Activation('gelu')(x)\n","    x = layers.Conv2D(32, kernel_size=3, strides=1, padding='same', activation=None)(x)\n","    x = layers.BatchNormalization()(x)\n","    x = layers.Activation('gelu')(x)\n","    return x\n","\n","# 2) Patches using tf.image.extract_patches\n","class Patches(layers.Layer):\n","    def __init__(self, patch_size):\n","        super().__init__()\n","        self.patch_size = patch_size\n","\n","    def call(self, images):\n","        # images: [batch, H, W, C]\n","        batch_size = tf.shape(images)[0]\n","        patches = tf.image.extract_patches(\n","            images=images,\n","            sizes=[1, self.patch_size, self.patch_size, 1],\n","            strides=[1, self.patch_size, self.patch_size, 1],\n","            rates=[1, 1, 1, 1],\n","            padding='VALID'\n","        )\n","        patch_dims = patches.shape[-1]\n","        patches = tf.reshape(patches, [batch_size, -1, patch_dims])\n","        return patches\n","\n","# 3) Patch encoding + Class token\n","class PatchEncoder(layers.Layer):\n","    def __init__(self, num_patches, projection_dim):\n","        super().__init__()\n","        self.num_patches = num_patches\n","        self.projection = layers.Dense(units=projection_dim)\n","        # +1 to accommodate class token position embedding\n","        self.position_embedding = layers.Embedding(input_dim=num_patches + 1, output_dim=projection_dim)\n","\n","    def build(self, input_shape):\n","        # create a trainable class token (1, 1, projection_dim)\n","        self.class_token = self.add_weight('class_token', shape=(1, 1, self.projection.units), initializer='zeros', trainable=True)\n","        super().build(input_shape)\n","\n","    def call(self, patch):\n","        # patch: [batch, num_patches, patch_dim]\n","        batch_size = tf.shape(patch)[0]\n","        projected = self.projection(patch)  # [batch, num_patches, projection_dim]\n","        class_tokens = tf.tile(self.class_token, [batch_size, 1, 1])  # [batch, 1, projection_dim]\n","        x = tf.concat([class_tokens, projected], axis=1)  # prepend class token\n","        positions = tf.range(start=0, limit=self.num_patches + 1, delta=1)\n","        encoded = x + self.position_embedding(positions)\n","        return encoded\n","\n","# 4) DropPath (stochastic depth) implementation\n","class DropPath(layers.Layer):\n","    def __init__(self, drop_prob=None):\n","        super().__init__()\n","        self.drop_prob = drop_prob if drop_prob is not None else 0.0\n","\n","    def call(self, x, training=False):\n","        if (not training) or (self.drop_prob == 0.0):\n","            return x\n","        keep_prob = 1.0 - self.drop_prob\n","        # shape for broadcasting: [batch, 1, 1]\n","        batch_size = tf.shape(x)[0]\n","        random_tensor = keep_prob + tf.random.uniform([batch_size, 1, 1], dtype=x.dtype)\n","        binary_tensor = tf.floor(random_tensor)\n","        x = tf.divide(x, keep_prob) * binary_tensor\n","        return x\n","\n","# MLP helper (used in transformer blocks and head)\n","def mlp(x, hidden_units, dropout_rate):\n","    for units in hidden_units:\n","        x = layers.Dense(units, activation=keras.activations.gelu)(x)\n","        x = layers.Dropout(dropout_rate)(x)\n","    return x\n","\n","# Create ViT with improvements\n","def create_improved_vit(\n","    input_shape=(32, 32, 3),\n","    image_size=IMAGE_SIZE,\n","    patch_size=PATCH_SIZE,\n","    projection_dim=128,\n","    num_heads=6,\n","    transformer_layers=10,\n","    transformer_mlp_units=None,\n","    mlp_head_units=None,\n","    drop_path_rate=0.1,\n","    num_classes=100\n","):\n","    if transformer_mlp_units is None:\n","        transformer_mlp_units = [projection_dim * 2, projection_dim]\n","    if mlp_head_units is None:\n","        mlp_head_units = [2048, 1024]\n","\n","    inputs = keras.Input(shape=input_shape)\n","\n","    # Data augmentation + conv stem\n","    x = DATA_AUG(inputs)\n","    x = conv_stem(x)  # conv stem - local feature extractor\n","\n","    # Create patches\n","    patches = Patches(patch_size)(x)\n","\n","    # Encode patches with class token + position embeddings\n","    encoded_patches = PatchEncoder(NUM_PATCHES, projection_dim)(patches)\n","\n","    # Transformer blocks with optional stochastic depth (DropPath)\n","    # We'll linearly scale the drop path across layers for better regularization\n","    dpr_rates = np.linspace(0.0, drop_path_rate, transformer_layers)\n","\n","    for i in range(transformer_layers):\n","        x1 = layers.LayerNormalization(epsilon=1e-6)(encoded_patches)\n","        attention_output = layers.MultiHeadAttention(num_heads=num_heads, key_dim=projection_dim, dropout=0.1)(x1, x1)\n","        attention_output = DropPath(dpr_rates[i])(attention_output, training=True)\n","        x2 = layers.Add()([attention_output, encoded_patches])\n","\n","        x3 = layers.LayerNormalization(epsilon=1e-6)(x2)\n","        x3 = mlp(x3, transformer_mlp_units, dropout_rate=0.1)\n","        x3 = DropPath(dpr_rates[i])(x3, training=True)\n","        encoded_patches = layers.Add()([x3, x2])\n","\n","    # Use the class token output (index 0)\n","    representation = layers.LayerNormalization(epsilon=1e-6)(encoded_patches)\n","    cls_token_output = representation[:, 0, :]\n","    x = layers.Dropout(0.5)(cls_token_output)\n","    x = mlp(x, mlp_head_units, dropout_rate=0.5)\n","    outputs = layers.Dense(num_classes, activation='softmax')(x)\n","\n","    model = keras.Model(inputs=inputs, outputs=outputs, name='improved_vit')\n","    return model\n","\n","# -----------------------------\n","# Training loop / experiments\n","# -----------------------------\n","\n","def compile_and_train(model, lr=1e-3, weight_decay=1e-4, epochs=30, steps_per_epoch=None):\n","    # Cosine decay schedule\n","    total_steps = (steps_per_epoch if steps_per_epoch is not None else math.ceil((x_train.shape[0]*0.9)/BATCH_SIZE)) * epochs\n","    # If tfa (tensorflow_addons) is available, prefer AdamW. Otherwise fallback to Adam.\n","    try:\n","        import tensorflow_addons as tfa\n","        print('tensorflow_addons found: using AdamW')\n","        optimizer = tfa.optimizers.AdamW(learning_rate=lr_schedule, weight_decay=weight_decay)\n","    except Exception:\n","        print('tensorflow_addons not found: falling back to Adam (without weight decay)')\n","        optimizer = keras.optimizers.Adam(learning_rate=lr)\n","\n","    lr_schedule = tf.keras.optimizers.schedules.CosineDecay(initial_learning_rate=lr, decay_steps=total_steps)\n","\n","    # Compile\n","    model.compile(\n","        optimizer=optimizer,\n","        loss=keras.losses.SparseCategoricalCrossentropy(),\n","        metrics=['accuracy']\n","    )\n","\n","    # Callbacks\n","    ckpt = keras.callbacks.ModelCheckpoint(os.path.join(OUTPUT_DIR, 'best_weights.h5'), monitor='val_accuracy', save_best_only=True, save_weights_only=True)\n","    early = keras.callbacks.EarlyStopping(monitor='val_accuracy', patience=8, restore_best_weights=True)\n","\n","    history = model.fit(\n","        train_ds,\n","        validation_data=val_ds,\n","        epochs=epochs,\n","        callbacks=[ckpt, early]\n","    )\n","    return history\n","\n","# -----------------------------\n","# Run experiments (grid) - Note: each experiment can be long. The code organizes experiments and saves results.\n","# -----------------------------\n","\n","if RUN_EXPERIMENTS:\n","    experiments = [\n","        {\n","            'name': 'baseline_vit',\n","            'projection_dim': 64,\n","            'transformer_layers': 8,\n","            'num_heads': 4,\n","            'drop_path_rate': 0.0,\n","            'batch_size': 256,\n","            'lr': 1e-3,\n","            'epochs': 20\n","        },\n","        {\n","            'name': 'convstem + class_token',\n","            'projection_dim': 64,\n","            'transformer_layers': 8,\n","            'num_heads': 4,\n","            'drop_path_rate': 0.05,\n","            'batch_size': 128,\n","            'lr': 1e-3,\n","            'epochs': 25\n","        },\n","        {\n","            'name': 'deeper + larger dim',\n","            'projection_dim': 128,\n","            'transformer_layers': 10,\n","            'num_heads': 6,\n","            'drop_path_rate': 0.1,\n","            'batch_size': 128,\n","            'lr': 1e-3,\n","            'epochs': 30\n","        }\n","    ]\n","\n","    results = []\n","    # For reproducibility, set seed\n","    tf.random.set_seed(42)\n","    np.random.seed(42)\n","\n","    for exp in experiments:\n","        print('\\n' + '='*40)\n","        print(f\"Running experiment: {exp['name']}\")\n","        print('='*40)\n","\n","        # Build model with given hyperparameters\n","        model = create_improved_vit(\n","            input_shape=(32, 32, 3),\n","            image_size=IMAGE_SIZE,\n","            patch_size=PATCH_SIZE,\n","            projection_dim=exp['projection_dim'],\n","            num_heads=exp['num_heads'],\n","            transformer_layers=exp['transformer_layers'],\n","            drop_path_rate=exp['drop_path_rate'],\n","            num_classes=num_classes\n","        )\n","\n","        model.summary()\n","\n","        # Compile & train\n","        # Adjust global BATCH_SIZE if needed\n","        history = compile_and_train(model, lr=exp['lr'], epochs=exp['epochs'])\n","\n","        # Plot history\n","        plot_history(history, title_prefix=exp['name'])\n","\n","        # Evaluate on test set\n","        test_loss, test_acc = model.evaluate(test_ds, verbose=2)\n","        print(f\"Test accuracy for {exp['name']}: {test_acc:.4f}\")\n","\n","        # Save history and results\n","        res = {\n","            'experiment': exp['name'],\n","            'projection_dim': exp['projection_dim'],\n","            'transformer_layers': exp['transformer_layers'],\n","            'num_heads': exp['num_heads'],\n","            'drop_path_rate': exp['drop_path_rate'],\n","            'batch_size': exp['batch_size'],\n","            'lr': exp['lr'],\n","            'epochs_trained': len(history.history['loss']),\n","            'test_accuracy': float(test_acc),\n","            'test_loss': float(test_loss)\n","        }\n","        results.append(res)\n","\n","        # Save model\n","        model.save(os.path.join(OUTPUT_DIR, f\"{exp['name']}_model\"))\n","\n","        # Save history to json\n","        with open(os.path.join(OUTPUT_DIR, f\"{exp['name']}_history.json\"), 'w') as f:\n","            json.dump(history.history, f)\n","\n","    # Save results table\n","    df = pd.DataFrame(results)\n","    df.to_csv(os.path.join(OUTPUT_DIR, 'experiment_results.csv'), index=False)\n","    print('\\nAll experiments finished. Summary:')\n","    print(df)\n","\n","    # Show comparison table\n","    print('\\nComparison table (sorted by test_accuracy):')\n","    print(df.sort_values('test_accuracy', ascending=False))\n","\n","else:\n","    print('RUN_EXPERIMENTS set to False. The script built the model only.')\n","\n","# End of file"]}]}